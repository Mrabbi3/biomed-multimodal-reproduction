# ğŸ§¬ Biomedical Multimodal AI Reproduction

**Reproducing and extending findings from "Towards Generalist Biomedical AI" (Med-PaLM M)**

[![Paper](https://img.shields.io/badge/Paper-arXiv%202307.14334-red)](https://arxiv.org/abs/2307.14334)
[![Python](https://img.shields.io/badge/Python-3.10+-blue)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Active%20Research-orange)]()

---

## Overview

This project reproduces key experiments from Google's **Med-PaLM Multimodal (Med-PaLM M)** paper â€” the first demonstration of a generalist biomedical AI system capable of interpreting clinical language, medical imaging, and genomics with a single set of model weights.

Since Med-PaLM M's architecture (PaLM-E) is not open-source, this reproduction leverages open-source multimodal models (LLaVA-Med, BLIP-2) to replicate the paper's core findings on publicly available datasets from **MultiMedBench**.

### Research Questions

1. **Can open-source multimodal models match Med-PaLM M's performance** on Medical Visual Question Answering (VQA) tasks?
2. **Does the one-shot exemplar prompting strategy** described in the paper improve performance in open-source settings?
3. **Is there evidence of zero-shot generalization** to unseen medical concepts when fine-tuning on multiple biomedical tasks?

---

## Paper Summary

| Aspect | Details |
|--------|---------|
| **Model** | Med-PaLM M â€” built on PaLM-E (PaLM LLM + ViT vision encoder) |
| **Scales** | 12B, 84B, 562B parameters |
| **Benchmark** | MultiMedBench â€” 14 tasks, 12 datasets, 1M+ samples |
| **Modalities** | Clinical text, radiology, pathology, dermatology, mammography, genomics |
| **Key Result** | Single model matches or exceeds specialist SOTA on all 14 tasks |
| **Clinical Eval** | Radiologists preferred Med-PaLM M reports over human reports in up to 40.5% of cases |

### Architecture at a Glance

```
Input Image â†’ ViT Encoder â†’ 256 Visual Tokens â”€â”
                                                 â”œâ†’ PaLM Language Model â†’ Generated Text
Task Instruction + Context â†’ Text Tokens â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The model uses **instruction task prompting** with a **text-only one-shot exemplar** â€” providing an example input-output pair where the image is replaced with a `<img>` placeholder. This preserves compute efficiency while conditioning the model's output format.

---

## Project Structure

```
biomed-multimodal-reproduction/
â”‚
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ setup.py                     # Package setup
â”œâ”€â”€ .env.example                 # Environment variable template
â”‚
â”œâ”€â”€ configs/                     # Experiment configurations
â”‚   â”œâ”€â”€ vqa_rad.yaml
â”‚   â”œâ”€â”€ slake_vqa.yaml
â”‚   â””â”€â”€ path_vqa.yaml
â”‚
â”œâ”€â”€ data/                        # Data loading & preprocessing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ download.py              # Dataset download scripts
â”‚   â”œâ”€â”€ vqa_rad_loader.py        # VQA-RAD dataset loader
â”‚   â”œâ”€â”€ slake_loader.py          # Slake-VQA dataset loader
â”‚   â””â”€â”€ preprocessing.py         # Image resize, normalization
â”‚
â”œâ”€â”€ models/                      # Model wrappers & adapters
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_model.py            # Abstract base class
â”‚   â”œâ”€â”€ llava_med_wrapper.py     # LLaVA-Med integration
â”‚   â””â”€â”€ blip2_wrapper.py         # BLIP-2 integration
â”‚
â”œâ”€â”€ training/                    # Fine-tuning pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ trainer.py               # Training loop
â”‚   â”œâ”€â”€ prompts.py               # Instruction templates (from paper)
â”‚   â””â”€â”€ multitask_mixer.py       # Task mixture sampling
â”‚
â”œâ”€â”€ evaluation/                  # Metrics & evaluation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics.py               # BLEU, ROUGE-L, F1 implementations
â”‚   â”œâ”€â”€ evaluate.py              # Full evaluation pipeline
â”‚   â””â”€â”€ compare_to_paper.py      # Side-by-side comparison with Table 2
â”‚
â”œâ”€â”€ experiments/                 # Experiment scripts
â”‚   â”œâ”€â”€ 01_data_sanity_check.py  # Phase 1: Verify data loading
â”‚   â”œâ”€â”€ 02_forward_pass_test.py  # Phase 2: Test model inference
â”‚   â”œâ”€â”€ 03_overfit_single_batch.py # Phase 3: Memorization test
â”‚   â”œâ”€â”€ 04_train_vqa.py          # Phase 4: Full training run
â”‚   â””â”€â”€ 05_zero_shot_eval.py     # Phase 5: Zero-shot generalization
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for exploration
â”‚   â”œâ”€â”€ 01_explore_datasets.ipynb
â”‚   â”œâ”€â”€ 02_model_playground.ipynb
â”‚   â””â”€â”€ 03_results_analysis.ipynb
â”‚
â”œâ”€â”€ results/                     # Experiment outputs
â”‚   â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ tables/
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ tests/                       # Unit tests
â”‚   â”œâ”€â”€ test_data_loader.py
â”‚   â”œâ”€â”€ test_metrics.py
â”‚   â””â”€â”€ test_model_forward.py
â”‚
â””â”€â”€ docs/                        # Additional documentation
    â”œâ”€â”€ SETUP_GUIDE.md
    â”œâ”€â”€ DATASET_ACCESS.md
    â””â”€â”€ REPRODUCTION_LOG.md
```

---

## Research Phases & Milestones

### Phase 1: Project Definition & Data Setup *(Weeks 1â€“2)*

| Task | Status | Deliverable |
|------|--------|-------------|
| Select target task(s): Medical VQA | â¬œ | Decision documented in `REPRODUCTION_LOG.md` |
| Download VQA-RAD dataset | â¬œ | `data/vqa_rad/` populated |
| Download Slake-VQA dataset | â¬œ | `data/slake/` populated |
| Build data loaders | â¬œ | `test_data_loader.py` passes |
| Run `01_data_sanity_check.py` | â¬œ | Verified: images match questions |

**Key Insight from Paper:** Images are resized to 224Ã—224Ã—3 with aspect ratio preserved via padding. Grayscale images are stacked to 3 channels.

### Phase 2: Model Selection & Baseline *(Weeks 3â€“4)*

| Task | Status | Deliverable |
|------|--------|-------------|
| Clone LLaVA-Med or BLIP-2 repo | â¬œ | Working model inference |
| Implement instruction prompting | â¬œ | `training/prompts.py` matches Figure 2 |
| Run `02_forward_pass_test.py` | â¬œ | Model produces text output |
| Establish baseline metrics (no fine-tuning) | â¬œ | Baseline numbers logged |

**Key Insight from Paper:** The one-shot exemplar uses a dummy `<img>` text placeholder instead of an actual image â€” this avoids cross-attention interference between multiple images.

### Phase 3: Training & Validation Pipeline *(Weeks 5â€“8)*

| Task | Status | Deliverable |
|------|--------|-------------|
| Implement BLEU-1 and F1 metrics | â¬œ | `test_metrics.py` passes |
| Run `03_overfit_single_batch.py` | â¬œ | Loss â†’ 0 on 5 examples |
| Fine-tune on VQA-RAD training set | â¬œ | Training curves in `results/` |
| Evaluate on VQA-RAD test set | â¬œ | Comparison table vs. Table 2 |

**Paper Baselines to Beat (VQA-RAD):**

| Model | BLEU-1 | F1 |
|-------|--------|----|
| Prior SOTA (specialist) | 71.03% | N/A |
| PaLM-E 84B (no fine-tuning) | 59.19% | 38.67% |
| Med-PaLM M 12B | 64.02% | 50.66% |
| Med-PaLM M 84B | 69.38% | 59.90% |
| **Med-PaLM M 562B** | **71.27%** | **62.06%** |

### Phase 4: Extended Experiments *(Month 3)*

| Task | Status | Deliverable |
|------|--------|-------------|
| Fine-tune on Slake-VQA | â¬œ | Cross-dataset comparison |
| Test one-shot exemplar ablation | â¬œ | With vs. without exemplar |
| Probe zero-shot generalization | â¬œ | Novel concept evaluation |
| Multi-task training (VQA-RAD + Slake) | â¬œ | Transfer learning analysis |

### Phase 5: Documentation & Portfolio *(Month 4)*

| Task | Status | Deliverable |
|------|--------|-------------|
| Write reproduction report | â¬œ | `docs/REPRODUCTION_LOG.md` |
| Create comparison tables | â¬œ | `results/tables/` |
| Generate qualitative examples | â¬œ | Model input â†’ output screenshots |
| Final README polish | â¬œ | Portfolio-ready repository |

---

## Quick Start

### Prerequisites

- Python 3.10+
- CUDA-capable GPU (8GB+ VRAM recommended)
- ~10GB disk space for datasets and model weights

### Installation

```bash
# Clone the repository
git clone https://github.com/Mrabbi3/biomed-multimodal-reproduction.git
cd biomed-multimodal-reproduction

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt
```

### Dataset Download

```bash
# VQA-RAD (small, publicly available â€” good starting point)
python data/download.py --dataset vqa_rad

# Slake-VQA
python data/download.py --dataset slake_vqa
```

### Run Sanity Check

```bash
# Verify data loading â€” displays sample image + question pair
python experiments/01_data_sanity_check.py
```

---

## Datasets Used

| Dataset | Task | Size | Access |
|---------|------|------|--------|
| **VQA-RAD** | Radiology VQA | 3,515 QA pairs, 315 images | Public |
| **Slake-VQA** | Radiology VQA (bilingual) | 14,028 QA pairs, 642 images | Public |
| **Path-VQA** | Pathology VQA | 32,799 QA pairs, 4,998 images | Public |
| MIMIC-CXR | CXR Report Generation | 377,110 images | Credentialed (PhysioNet) |

*This reproduction focuses on the first three VQA datasets for accessibility. MIMIC-CXR is an optional extension.*

---

## Evaluation Metrics

Following the paper's methodology:

- **BLEU-1**: Unigram precision between generated and reference answers
- **F1 (Token-level)**: Harmonic mean of token-level precision and recall
- **ROUGE-L**: Longest common subsequence overlap (for report generation)

> **Note from paper:** The authors use open-ended generative evaluation rather than classification accuracy, since their model generates free-form text. This is more challenging but better captures "near misses."

---

## Key Findings from the Paper

### What This Project Aims to Reproduce

1. **Generalist â‰¥ Specialist**: A single model with one set of weights can match task-specific models across multiple biomedical tasks (Table 2).

2. **Domain Fine-tuning Matters**: PaLM-E without biomedical fine-tuning scores 38.67% F1 on VQA-RAD vs. 62.06% after fine-tuning â€” a massive improvement from domain adaptation.

3. **Scaling Benefits Language Tasks Most**: Medical QA improves dramatically with scale (29% â†’ 70% on MedQA), while image classification plateaus when the vision encoder isn't scaled.

4. **Positive Task Transfer**: Training on both CXR report generation AND classification simultaneously improves both tasks compared to training on either alone (Table 6).

5. **Zero-shot Generalization**: Med-PaLM M detects tuberculosis from chest X-rays at 87.68% accuracy despite never being trained on TB labels (Table 4).

---

## Differences from Original Paper

| Aspect | Med-PaLM M (Paper) | This Reproduction |
|--------|--------------------|--------------------|
| Base model | PaLM-E (proprietary) | LLaVA-Med / BLIP-2 (open-source) |
| Scale | 12Bâ€“562B params | ~7Bâ€“13B params |
| Training data | Full MultiMedBench (1M+) | VQA subset (~50K samples) |
| Compute | TPU pods | Single GPU (consumer) |
| Tasks | 14 simultaneous tasks | 2â€“3 VQA tasks |
| Evaluation | Automated + radiologist review | Automated metrics |

---

## References

```bibtex
@article{tu2023towards,
  title={Towards Generalist Biomedical AI},
  author={Tu, Tao and Azizi, Shekoofeh and Driess, Danny and others},
  journal={arXiv preprint arXiv:2307.14334},
  year={2023}
}
```

**Related Open-Source Projects:**
- [LLaVA-Med](https://github.com/microsoft/LLaVA-Med) â€” Medical visual instruction tuning
- [BLIP-2](https://github.com/salesforce/LAVIS) â€” Bootstrapped language-image pretraining
- [OpenFlamingo](https://github.com/mlfoundations/open_flamingo) â€” Open-source Flamingo reproduction

---

## Author

**MD Rabbi** â€” Computer Science Student & Aspiring AI/ML Engineer

*This project is part of an independent research initiative in biomedical multimodal AI, inspired by the Med-PaLM M paper from Google Research & Google DeepMind.*

---

## License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

*Note: Datasets used in this project have their own licensing terms. Please review individual dataset licenses before use.*
