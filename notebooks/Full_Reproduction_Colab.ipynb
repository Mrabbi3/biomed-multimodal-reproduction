{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß¨ Reproducing Med-PaLM M: Towards Generalist Biomedical AI\n",
        "\n",
        "**Paper:** Tu et al., \"Towards Generalist Biomedical AI\" (arXiv:2307.14334)\n",
        "\n",
        "**What this notebook does:**\n",
        "1. Clones the project repo\n",
        "2. Downloads the VQA-RAD dataset\n",
        "3. Runs data sanity checks\n",
        "4. Establishes zero-shot baseline (BLIP-2 without training)\n",
        "5. Verifies training pipeline (overfit test)\n",
        "6. Fine-tunes on VQA-RAD and evaluates against paper baselines\n",
        "7. Runs generalization experiments\n",
        "8. Generates final comparison table\n",
        "\n",
        "**Requirements:** GPU runtime (T4 is sufficient)\n",
        "\n",
        "---\n",
        "\n",
        "‚ö†Ô∏è **FIRST: Enable GPU** ‚Üí Runtime ‚Üí Change runtime type ‚Üí T4 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Verify GPU and Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verify GPU is available\n",
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå NO GPU DETECTED ‚Äî Go to Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/Mrabbi3/biomed-multimodal-reproduction.git\n",
        "%cd biomed-multimodal-reproduction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers>=4.36.0 accelerate>=0.25.0 peft>=0.7.0 \\\n",
        "    bitsandbytes datasets Pillow tqdm pyyaml nltk rouge-score \\\n",
        "    matplotlib seaborn evaluate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 1: Download Data & Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Download VQA-RAD dataset (~50 MB)\n",
        "!python data/download.py --dataset vqa_rad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run sanity checks ‚Äî verifies data loads, preprocessing works, metrics are correct\n",
        "!python experiments/01_data_sanity_check.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize the sanity check output\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "if os.path.exists(\"results/figures/vqa_rad_sanity_check.png\"):\n",
        "    display(Image(filename=\"results/figures/vqa_rad_sanity_check.png\"))\n",
        "    print(\"‚úì Images and questions look correct!\")\n",
        "else:\n",
        "    print(\"Sanity check image not generated ‚Äî check errors above\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 2: Zero-Shot Baseline (No Training)\n",
        "\n",
        "We load BLIP-2 and test it on VQA-RAD **without any fine-tuning**.\n",
        "This is comparable to the paper's PaLM-E 84B baseline (BLEU-1: 59.19%)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run forward pass test ‚Äî establishes zero-shot baseline\n",
        "# This downloads BLIP-2 (~7GB) on first run\n",
        "!python experiments/02_forward_pass_test.py --model blip2 --max_samples 50 --quantize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# View the baseline results\n",
        "import json\n",
        "if os.path.exists(\"results/tables/baseline_metrics.json\"):\n",
        "    with open(\"results/tables/baseline_metrics.json\") as f:\n",
        "        baseline = json.load(f)\n",
        "    print(f\"Zero-Shot Baseline Results:\")\n",
        "    print(f\"  BLEU-1: {baseline['bleu_1']:.2f}%\")\n",
        "    print(f\"  F1:     {baseline['f1']:.2f}%\")\n",
        "    print(f\"\\nPaper comparison:\")\n",
        "    print(f\"  PaLM-E 84B (no finetune): BLEU-1=59.19%, F1=38.67%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 3: Overfit Test (Verify Training Works)\n",
        "\n",
        "Before real training, we memorize 5 examples to verify:\n",
        "- LoRA adapters are applied correctly\n",
        "- Gradients flow through the model\n",
        "- Loss decreases toward zero\n",
        "\n",
        "If this fails, there's a bug. If it passes, the pipeline is trustworthy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Overfit 5 examples for 50 epochs ‚Äî should memorize them\n",
        "!python experiments/03_overfit_single_batch.py --num_samples 5 --epochs 50 --quantize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 4: Full Training & Evaluation üöÄ\n",
        "\n",
        "This is the main experiment. We fine-tune BLIP-2 on the VQA-RAD training set\n",
        "using LoRA, then evaluate on the test set and compare to Med-PaLM M baselines.\n",
        "\n",
        "**Expected time:** ~20-40 minutes on a T4 GPU for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Full training run\n",
        "!python experiments/04_train_vqa.py \\\n",
        "    --dataset vqa_rad \\\n",
        "    --epochs 10 \\\n",
        "    --batch_size 4 \\\n",
        "    --lr 5e-5 \\\n",
        "    --lora_rank 16 \\\n",
        "    --grad_accum 4 \\\n",
        "    --quantize \\\n",
        "    --use_exemplar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# View training log\n",
        "if os.path.exists(\"results/logs/training_log.json\"):\n",
        "    with open(\"results/logs/training_log.json\") as f:\n",
        "        log = json.load(f)\n",
        "    \n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    epochs = [e[\"epoch\"] for e in log]\n",
        "    train_loss = [e[\"train_loss\"] for e in log]\n",
        "    \n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(epochs, train_loss, 'b-o', label='Train Loss')\n",
        "    \n",
        "    if \"val_loss\" in log[0]:\n",
        "        val_loss = [e[\"val_loss\"] for e in log]\n",
        "        plt.plot(epochs, val_loss, 'r-o', label='Val Loss')\n",
        "    \n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Progress')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results/figures/training_curve.png', dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"Final train loss: {train_loss[-1]:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# View evaluation results vs paper baselines\n",
        "if os.path.exists(\"results/tables/vqa_rad_metrics.json\"):\n",
        "    with open(\"results/tables/vqa_rad_metrics.json\") as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"FINE-TUNED RESULTS vs PAPER BASELINES (VQA-RAD)\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    comparison = [\n",
        "        (\"Prior SOTA (specialist)\",    71.03, None),\n",
        "        (\"PaLM-E 84B (no finetune)\",   59.19, 38.67),\n",
        "        (\"Med-PaLM M 12B\",             64.02, 50.66),\n",
        "        (\"Med-PaLM M 84B\",             69.38, 59.90),\n",
        "        (\"Med-PaLM M 562B\",            71.27, 62.06),\n",
        "        (\"Ours (BLIP-2 + LoRA)\",       results['bleu_1'], results['f1']),\n",
        "    ]\n",
        "    \n",
        "    print(f\"{'Model':<30} {'BLEU-1':>10} {'F1':>10}\")\n",
        "    print(\"-\" * 50)\n",
        "    for name, bleu, f1 in comparison:\n",
        "        b = f\"{bleu:.2f}%\" if bleu else \"N/A\"\n",
        "        f = f\"{f1:.2f}%\" if f1 else \"N/A\"\n",
        "        marker = \" ‚Üê US\" if \"Ours\" in name else \"\"\n",
        "        print(f\"{name:<30} {b:>10} {f:>10}{marker}\")\n",
        "    print(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 5: Generalization Experiments\n",
        "\n",
        "These experiments test the paper's key claims:\n",
        "1. **Cross-dataset transfer** ‚Äî Does fine-tuning on VQA-RAD help on Slake-VQA?\n",
        "2. **Exemplar ablation** ‚Äî Does the one-shot prompting trick actually help?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Download Slake-VQA for cross-dataset testing\n",
        "!python data/download.py --dataset slake_vqa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run all generalization experiments\n",
        "!python experiments/05_zero_shot_eval.py --experiment all --max_samples 100 --quantize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# View exemplar ablation results\n",
        "if os.path.exists(\"results/tables/exemplar_ablation.json\"):\n",
        "    with open(\"results/tables/exemplar_ablation.json\") as f:\n",
        "        ablation = json.load(f)\n",
        "    \n",
        "    print(\"ONE-SHOT EXEMPLAR ABLATION\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"{'Mode':<25} {'BLEU-1':>10} {'F1':>10}\")\n",
        "    print(\"-\" * 45)\n",
        "    for mode, m in ablation.items():\n",
        "        print(f\"{mode:<25} {m['bleu_1']:>9.2f}% {m['f1']:>9.2f}%\")\n",
        "    \n",
        "    diff = ablation['with_exemplar']['bleu_1'] - ablation['without_exemplar']['bleu_1']\n",
        "    print(f\"\\nExemplar effect: {diff:+.2f}% BLEU-1\")\n",
        "    if diff > 0:\n",
        "        print(\"‚Üí Exemplar HELPS (confirms paper's approach)\")\n",
        "    else:\n",
        "        print(\"‚Üí Exemplar did not help (interesting finding for our model)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Final: Generate Complete Comparison Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate the unified comparison table and bar charts\n",
        "!python evaluation/compare_to_paper.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display comparison chart\n",
        "from IPython.display import Image, display\n",
        "import glob\n",
        "\n",
        "for fig_path in glob.glob(\"results/figures/*_comparison.png\"):\n",
        "    print(f\"\\n{fig_path}:\")\n",
        "    display(Image(filename=fig_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Show the final markdown comparison table\n",
        "if os.path.exists(\"results/tables/full_comparison.md\"):\n",
        "    with open(\"results/tables/full_comparison.md\") as f:\n",
        "        print(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üì• Download Results\n",
        "\n",
        "Run this cell to zip all results for download."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Package results for download\n",
        "!tar -czf /content/reproduction_results.tar.gz results/\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/reproduction_results.tar.gz')\n",
        "print(\"‚úì Results downloaded! Add these to your GitHub repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "### What We Reproduced\n",
        "- Med-PaLM M's medical VQA methodology using open-source models\n",
        "- Instruction task prompting with one-shot exemplars\n",
        "- Domain-specific fine-tuning and its impact on performance\n",
        "- Cross-dataset generalization evaluation\n",
        "\n",
        "### Key Differences from Original Paper\n",
        "| Aspect | Med-PaLM M | Our Reproduction |\n",
        "|--------|-----------|------------------|\n",
        "| Model | PaLM-E (562B) | BLIP-2 (~3B) |\n",
        "| Training | Full fine-tuning on TPU pods | LoRA on single GPU |\n",
        "| Data | 1M+ samples across 14 tasks | ~3.5K-14K VQA samples |\n",
        "| Compute | Weeks on TPU v4 | ~30 min on T4 GPU |"
      ]
    }
  ]
}