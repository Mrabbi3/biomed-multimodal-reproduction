{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß¨ Reproducing Med-PaLM M: Towards Generalist Biomedical AI\n",
        "\n",
        "**Paper:** Tu et al., \"Towards Generalist Biomedical AI\" (arXiv:2307.14334)\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANT:** Run cells ONE AT A TIME from top to bottom. Wait for each cell to finish (spinner stops) before running the next one.\n",
        "\n",
        "‚ö†Ô∏è **FIRST:** Enable GPU ‚Üí Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0: Verify GPU and Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verify GPU is available\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"VRAM: {vram:.1f} GB\")\n",
        "    print(\"\\n‚úÖ GPU is ready!\")\n",
        "else:\n",
        "    print(\"\\n‚ùå NO GPU ‚Äî Go to Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Clone the repository\n",
        "!rm -rf biomed-multimodal-reproduction\n",
        "!git clone https://github.com/Mrabbi3/biomed-multimodal-reproduction.git\n",
        "%cd biomed-multimodal-reproduction\n",
        "print(\"\\n‚úÖ Repository cloned!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies (takes ~2 minutes)\n",
        "!pip install -q transformers>=4.36.0 accelerate>=0.25.0 peft>=0.7.0 \\\n",
        "    bitsandbytes datasets Pillow tqdm pyyaml nltk rouge-score \\\n",
        "    matplotlib seaborn evaluate\n",
        "print(\"\\n‚úÖ Dependencies installed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 1: Download Data & Sanity Check\n",
        "\n",
        "Downloads VQA-RAD (~50 MB) and verifies everything loads correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Download VQA-RAD dataset (~50 MB)\n",
        "!python data/download.py --dataset vqa_rad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run sanity checks\n",
        "!python experiments/01_data_sanity_check.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize the sanity check output\n",
        "import os\n",
        "from IPython.display import Image, display\n",
        "\n",
        "fig_path = \"results/figures/vqa_rad_sanity_check.png\"\n",
        "if os.path.exists(fig_path):\n",
        "    display(Image(filename=fig_path))\n",
        "    print(\"‚úÖ Images and questions look correct!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Sanity check image not generated ‚Äî check errors above\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 2: Zero-Shot Baseline (No Training)\n",
        "\n",
        "Load BLIP-2 and test on VQA-RAD **without any fine-tuning**.\n",
        "Comparable to the paper's PaLM-E 84B baseline (BLEU-1: 59.19%).\n",
        "\n",
        "‚è±Ô∏è **~10 minutes** (downloads ~7GB model on first run)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run forward pass test ‚Äî establishes zero-shot baseline\n",
        "!python experiments/02_forward_pass_test.py --model blip2 --max_samples 50 --quantize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# View the baseline results\n",
        "import os\n",
        "import json\n",
        "\n",
        "baseline_path = \"results/tables/baseline_metrics.json\"\n",
        "if os.path.exists(baseline_path):\n",
        "    with open(baseline_path) as f:\n",
        "        baseline = json.load(f)\n",
        "    print(\"ZERO-SHOT BASELINE RESULTS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"  BLEU-1: {baseline['bleu_1']:.2f}%\")\n",
        "    print(f\"  F1:     {baseline['f1']:.2f}%\")\n",
        "    print(f\"\\nPaper comparison:\")\n",
        "    print(f\"  PaLM-E 84B (no finetune): BLEU-1=59.19%, F1=38.67%\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No baseline results yet ‚Äî run the cell above first\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 3: Overfit Test (Verify Training Works)\n",
        "\n",
        "Before real training, we memorize 5 examples to verify the pipeline:\n",
        "- LoRA adapters are applied correctly\n",
        "- Gradients flow through the model\n",
        "- Loss decreases toward zero\n",
        "\n",
        "If this fails ‚Üí bug in code. If it passes ‚Üí safe to do full training.\n",
        "\n",
        "‚è±Ô∏è **~5 minutes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Overfit 5 examples for 50 epochs\n",
        "!python experiments/03_overfit_single_batch.py --num_samples 5 --epochs 50 --quantize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 4: Full Training & Evaluation üöÄ\n",
        "\n",
        "Fine-tune BLIP-2 on VQA-RAD training set using LoRA, then evaluate\n",
        "on the test set and compare to Med-PaLM M baselines.\n",
        "\n",
        "‚è±Ô∏è **~20-40 minutes on T4 GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Full training run\n",
        "!python experiments/04_train_vqa.py \\\n",
        "    --dataset vqa_rad \\\n",
        "    --epochs 10 \\\n",
        "    --batch_size 4 \\\n",
        "    --lr 5e-5 \\\n",
        "    --lora_rank 16 \\\n",
        "    --grad_accum 4 \\\n",
        "    --quantize \\\n",
        "    --use_exemplar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# View training curve\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "log_path = \"results/logs/training_log.json\"\n",
        "if os.path.exists(log_path):\n",
        "    with open(log_path) as f:\n",
        "        log = json.load(f)\n",
        "\n",
        "    epochs = [e[\"epoch\"] for e in log]\n",
        "    train_loss = [e[\"train_loss\"] for e in log]\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(epochs, train_loss, 'b-o', label='Train Loss')\n",
        "\n",
        "    if \"val_loss\" in log[0]:\n",
        "        val_loss = [e[\"val_loss\"] for e in log]\n",
        "        plt.plot(epochs, val_loss, 'r-o', label='Val Loss')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Progress')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results/figures/training_curve.png', dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"Final train loss: {train_loss[-1]:.4f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No training log yet ‚Äî run the training cell above first\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# View evaluation results vs paper baselines\n",
        "import os\n",
        "import json\n",
        "\n",
        "metrics_path = \"results/tables/vqa_rad_metrics.json\"\n",
        "if os.path.exists(metrics_path):\n",
        "    with open(metrics_path) as f:\n",
        "        results = json.load(f)\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"FINE-TUNED RESULTS vs PAPER BASELINES (VQA-RAD)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    comparison = [\n",
        "        (\"Prior SOTA (specialist)\",    71.03, None),\n",
        "        (\"PaLM-E 84B (no finetune)\",   59.19, 38.67),\n",
        "        (\"Med-PaLM M 12B\",             64.02, 50.66),\n",
        "        (\"Med-PaLM M 84B\",             69.38, 59.90),\n",
        "        (\"Med-PaLM M 562B\",            71.27, 62.06),\n",
        "        (\"Ours (BLIP-2 + LoRA)\",       results['bleu_1'], results['f1']),\n",
        "    ]\n",
        "\n",
        "    print(f\"{'Model':<30} {'BLEU-1':>10} {'F1':>10}\")\n",
        "    print(\"-\" * 50)\n",
        "    for name, bleu, f1 in comparison:\n",
        "        b = f\"{bleu:.2f}%\" if bleu else \"N/A\"\n",
        "        f = f\"{f1:.2f}%\" if f1 else \"N/A\"\n",
        "        marker = \" ‚Üê US\" if \"Ours\" in name else \"\"\n",
        "        print(f\"{name:<30} {b:>10} {f:>10}{marker}\")\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No evaluation results yet ‚Äî run training cell above first\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 5: Generalization Experiments\n",
        "\n",
        "Tests the paper's key claims:\n",
        "1. **Cross-dataset transfer** ‚Äî Does fine-tuning on VQA-RAD help on Slake-VQA?\n",
        "2. **Exemplar ablation** ‚Äî Does the one-shot prompting trick actually help?\n",
        "\n",
        "‚è±Ô∏è **~15 minutes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Download Slake-VQA for cross-dataset testing\n",
        "!python data/download.py --dataset slake_vqa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run all generalization experiments\n",
        "!python experiments/05_zero_shot_eval.py --experiment all --max_samples 100 --quantize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# View exemplar ablation results\n",
        "import os\n",
        "import json\n",
        "\n",
        "ablation_path = \"results/tables/exemplar_ablation.json\"\n",
        "if os.path.exists(ablation_path):\n",
        "    with open(ablation_path) as f:\n",
        "        ablation = json.load(f)\n",
        "\n",
        "    print(\"ONE-SHOT EXEMPLAR ABLATION\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"{'Mode':<25} {'BLEU-1':>10} {'F1':>10}\")\n",
        "    print(\"-\" * 45)\n",
        "    for mode, m in ablation.items():\n",
        "        print(f\"{mode:<25} {m['bleu_1']:>9.2f}% {m['f1']:>9.2f}%\")\n",
        "\n",
        "    diff = ablation['with_exemplar']['bleu_1'] - ablation['without_exemplar']['bleu_1']\n",
        "    print(f\"\\nExemplar effect: {diff:+.2f}% BLEU-1\")\n",
        "    if diff > 0:\n",
        "        print(\"‚Üí Exemplar HELPS (confirms paper's approach)\")\n",
        "    else:\n",
        "        print(\"‚Üí Exemplar did not help (interesting finding for our model)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No ablation results yet ‚Äî run the cell above first\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Final: Generate Complete Comparison Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate the unified comparison table and bar charts\n",
        "!python evaluation/compare_to_paper.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display comparison charts\n",
        "import os\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "charts = glob.glob(\"results/figures/*_comparison.png\")\n",
        "if charts:\n",
        "    for fig_path in charts:\n",
        "        print(f\"\\n{fig_path}:\")\n",
        "        display(Image(filename=fig_path))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No comparison charts generated yet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Show the final markdown comparison table\n",
        "import os\n",
        "\n",
        "table_path = \"results/tables/full_comparison.md\"\n",
        "if os.path.exists(table_path):\n",
        "    with open(table_path) as f:\n",
        "        print(f.read())\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No comparison table yet ‚Äî run the cell above first\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üì• Download Results\n",
        "\n",
        "Downloads all results (metrics, charts, predictions) as a zip file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Package results for download\n",
        "import os\n",
        "!tar -czf /content/reproduction_results.tar.gz results/\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/reproduction_results.tar.gz')\n",
        "print(\"‚úÖ Results downloaded! Add these to your GitHub repo.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "### What We Reproduced\n",
        "- Med-PaLM M's medical VQA methodology using open-source models\n",
        "- Instruction task prompting with one-shot exemplars\n",
        "- Domain-specific fine-tuning and its impact on performance\n",
        "- Cross-dataset generalization evaluation\n",
        "\n",
        "### Key Differences from Original Paper\n",
        "| Aspect | Med-PaLM M | Our Reproduction |\n",
        "|--------|-----------|------------------|\n",
        "| Model | PaLM-E (562B) | BLIP-2 (~3B) |\n",
        "| Training | Full fine-tuning on TPU pods | LoRA on single GPU |\n",
        "| Data | 1M+ samples across 14 tasks | ~3.5K-14K VQA samples |\n",
        "| Compute | Weeks on TPU v4 | ~1 hour on T4 GPU |"
      ]
    }
  ]
}