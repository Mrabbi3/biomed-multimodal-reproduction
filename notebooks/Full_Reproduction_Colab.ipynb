{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reproducing Med-PaLM M: An Open-Source Approach to Generalist Biomedical AI\n",
        "\n",
        "**Paper:** Tu et al., \"Towards Generalist Biomedical AI\" ([arXiv:2307.14334](https://arxiv.org/abs/2307.14334))\n",
        "\n",
        "**Author:** MD Rabbi ¬∑ Department of Computer Science ¬∑ February 2026\n",
        "\n",
        "This notebook reproduces key experiments from Google's Med-PaLM M paper using open-source models.\n",
        "We replace PaLM-E (562B) with BLIP-2 (3.4B) and implement the paper's complete medical VQA pipeline.\n",
        "\n",
        "‚ö†Ô∏è **Before running:** Go to **Runtime ‚Üí Change runtime type ‚Üí T4 GPU**\n",
        "\n",
        "‚è±Ô∏è **Total time:** ~1 hour\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title **Setup: GPU Check, Clone Repo, Install Dependencies** (run this first, ~3 min)\n",
        "import torch, os, json\n",
        "\n",
        "# GPU check\n",
        "assert torch.cuda.is_available(), \"‚ùå No GPU! Go to Runtime ‚Üí Change runtime type ‚Üí T4 GPU\"\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "print(f\"‚úÖ GPU: {gpu_name} ({vram:.1f} GB VRAM)\")\n",
        "\n",
        "# Clone repo\n",
        "!rm -rf biomed-multimodal-reproduction\n",
        "!git clone -q https://github.com/Mrabbi3/biomed-multimodal-reproduction.git\n",
        "%cd biomed-multimodal-reproduction\n",
        "print(\"‚úÖ Repository cloned\")\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q transformers>=4.36.0 accelerate>=0.25.0 peft>=0.7.0 \\\n",
        "    bitsandbytes datasets Pillow tqdm pyyaml nltk rouge-score matplotlib seaborn evaluate\n",
        "print(\"‚úÖ Dependencies installed\")\n",
        "\n",
        "# Patch BLIP-2 wrapper for newer transformers (BitsAndBytesConfig instead of load_in_8bit kwarg)\n",
        "patch = '''\"\"\"BLIP-2 Model Wrapper for Medical VQA.\"\"\"\n",
        "import torch\n",
        "from PIL import Image\n",
        "from .base_model import BaseBiomedModel\n",
        "\n",
        "class BLIP2Wrapper(BaseBiomedModel):\n",
        "    def __init__(self, model_name=\"Salesforce/blip2-flan-t5-xl\", device=None, load_in_8bit=False):\n",
        "        if device is None:\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        super().__init__(model_name, device)\n",
        "        self.load_in_8bit = load_in_8bit\n",
        "\n",
        "    def load_model(self):\n",
        "        from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "        print(f\"Loading BLIP-2: {self.model_name}\")\n",
        "        self.processor = Blip2Processor.from_pretrained(self.model_name)\n",
        "        load_kwargs = {}\n",
        "        if self.load_in_8bit and self.device == \"cuda\":\n",
        "            from transformers import BitsAndBytesConfig\n",
        "            load_kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_8bit=True)\n",
        "            load_kwargs[\"device_map\"] = \"auto\"\n",
        "        else:\n",
        "            load_kwargs[\"torch_dtype\"] = torch.float16 if self.device == \"cuda\" else torch.float32\n",
        "        self.model = Blip2ForConditionalGeneration.from_pretrained(self.model_name, **load_kwargs)\n",
        "        if not self.load_in_8bit and self.device == \"cuda\":\n",
        "            self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        param_count = sum(p.numel() for p in self.model.parameters()) / 1e9\n",
        "        print(f\"Model loaded ({param_count:.1f}B parameters)\")\n",
        "\n",
        "    def generate(self, image, prompt, max_new_tokens=256, temperature=1.0, num_beams=5):\n",
        "        if self.model is None: raise RuntimeError(\"Call load_model() first\")\n",
        "        if image.mode != \"RGB\": image = image.convert(\"RGB\")\n",
        "        inputs = self.processor(images=image, text=prompt, return_tensors=\"pt\")\n",
        "        if not self.load_in_8bit:\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "        else:\n",
        "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(**inputs, max_new_tokens=max_new_tokens, temperature=temperature, num_beams=num_beams)\n",
        "        return self.processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "    def generate_batch(self, images, prompts, max_new_tokens=256):\n",
        "        if self.model is None: raise RuntimeError(\"Call load_model() first\")\n",
        "        rgb_images = [img.convert(\"RGB\") if img.mode != \"RGB\" else img for img in images]\n",
        "        inputs = self.processor(images=rgb_images, text=prompts, return_tensors=\"pt\", padding=True)\n",
        "        if not self.load_in_8bit:\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "        else:\n",
        "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "        return [t.strip() for t in self.processor.batch_decode(outputs, skip_special_tokens=True)]\n",
        "'''\n",
        "with open('models/blip2_wrapper.py', 'w') as f:\n",
        "    f.write(patch)\n",
        "print(\"‚úÖ BLIP-2 wrapper patched for Colab\")\n",
        "print(\"\\nüöÄ Setup complete! Run cells below in order.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 1: Download Data & Sanity Check (~2 min)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title **Phase 1: Download VQA-RAD + Run Sanity Checks**\n",
        "!python data/download.py --dataset vqa_rad\n",
        "print(\"---\")\n",
        "!python experiments/01_data_sanity_check.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title **Visualize: Sample Medical Images from VQA-RAD**\n",
        "import os\n",
        "from IPython.display import Image, display\n",
        "fig_path = \"results/figures/vqa_rad_sanity_check.png\"\n",
        "if os.path.exists(fig_path):\n",
        "    display(Image(filename=fig_path, width=800))\n",
        "    print(\"‚úÖ Data loaded correctly ‚Äî images match their questions\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Image not generated\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 2: Zero-Shot Baseline (~10 min)\n",
        "\n",
        "Load BLIP-2 and test on VQA-RAD **without any training**.\n",
        "This downloads the 15GB model on first run. Be patient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title **Phase 2: Zero-Shot Forward Pass (downloads ~15GB model)**\n",
        "!python experiments/02_forward_pass_test.py --model blip2 --max_samples 50 --quantize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title **View: Zero-Shot Baseline Results**\n",
        "import os, json\n",
        "path = \"results/tables/baseline_metrics.json\"\n",
        "if os.path.exists(path):\n",
        "    with open(path) as f:\n",
        "        b = json.load(f)\n",
        "    print(\"ZERO-SHOT BASELINE (no training)\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"  Our BLIP-2 (3.4B):     BLEU-1 = {b['bleu_1']:.2f}%  |  F1 = {b['f1']:.2f}%\")\n",
        "    print(f\"  PaLM-E 84B (paper):    BLEU-1 = 59.19%  |  F1 = 38.67%\")\n",
        "    print(f\"  Med-PaLM M 562B:       BLEU-1 = 71.27%  |  F1 = 62.06%\")\n",
        "    print(f\"\\n  Gap explained by 165x parameter difference (3.4B vs 562B)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Run Phase 2 first\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 3: Overfit Test ‚Äî Verify Training Pipeline (~5 min)\n",
        "\n",
        "Train on 5 examples for 50 epochs. If the model can memorize them, the pipeline works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title **Phase 3: Overfit 5 Examples (Training Pipeline Validation)**\n",
        "!python experiments/03_overfit_single_batch.py --num_samples 5 --epochs 50 --quantize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 4: Full Training & Evaluation üöÄ (~30-40 min)\n",
        "\n",
        "Fine-tune BLIP-2 with LoRA on 1,793 VQA-RAD training samples.\n",
        "Evaluate on 451 test samples. Compare to Med-PaLM M baselines.\n",
        "\n",
        "**This is the main experiment. Let it run to completion.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title **Phase 4: Full Training Run** ‚è±Ô∏è ~30-40 minutes\n",
        "!python experiments/04_train_vqa.py \\\n",
        "    --dataset vqa_rad \\\n",
        "    --epochs 10 \\\n",
        "    --batch_size 4 \\\n",
        "    --lr 5e-5 \\\n",
        "    --lora_rank 16 \\\n",
        "    --grad_accum 4 \\\n",
        "    --quantize \\\n",
        "    --use_exemplar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title **View: Training Curve**\n",
        "import os, json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "log_path = \"results/logs/training_log.json\"\n",
        "if os.path.exists(log_path):\n",
        "    with open(log_path) as f:\n",
        "        log = json.load(f)\n",
        "    epochs = [e[\"epoch\"] for e in log]\n",
        "    train_loss = [e[\"train_loss\"] for e in log]\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(epochs, train_loss, 'b-o', label='Train Loss', linewidth=2)\n",
        "    if \"val_loss\" in log[0]:\n",
        "        plt.plot(epochs, [e[\"val_loss\"] for e in log], 'r-o', label='Val Loss', linewidth=2)\n",
        "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Training Progress')\n",
        "    plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()\n",
        "    plt.savefig('results/figures/training_curve.png', dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"Final loss: {train_loss[-1]:.4f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No training log ‚Äî run Phase 4 first\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title **View: Fine-Tuned Results vs Paper Baselines**\n",
        "import os, json\n",
        "\n",
        "# Check for fine-tuned results first, fall back to baseline\n",
        "for path in [\"results/tables/vqa_rad_metrics.json\", \"results/tables/baseline_metrics.json\"]:\n",
        "    if os.path.exists(path):\n",
        "        with open(path) as f:\n",
        "            results = json.load(f)\n",
        "        break\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No results found\"); results = None\n",
        "\n",
        "if results:\n",
        "    mode = results.get('mode', 'fine-tuned')\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"RESULTS vs PAPER BASELINES (VQA-RAD) [{mode}]\")\n",
        "    print(\"=\" * 60)\n",
        "    rows = [\n",
        "        (\"Prior SOTA (specialist)\",    71.03, None),\n",
        "        (\"PaLM-E 84B (zero-shot)\",     59.19, 38.67),\n",
        "        (\"Med-PaLM M 12B\",             64.02, 50.66),\n",
        "        (\"Med-PaLM M 84B\",             69.38, 59.90),\n",
        "        (\"Med-PaLM M 562B\",            71.27, 62.06),\n",
        "        (\"Ours (BLIP-2 + LoRA)\",       results['bleu_1'], results['f1']),\n",
        "    ]\n",
        "    print(f\"{'Model':<30} {'BLEU-1':>10} {'F1':>10}\")\n",
        "    print(\"-\" * 50)\n",
        "    for name, bleu, f1 in rows:\n",
        "        b = f\"{bleu:.2f}%\" if bleu else \"N/A\"\n",
        "        fv = f\"{f1:.2f}%\" if f1 else \"N/A\"\n",
        "        marker = \" ‚óÄ OURS\" if \"Ours\" in name else \"\"\n",
        "        print(f\"{name:<30} {b:>10} {fv:>10}{marker}\")\n",
        "    print(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Phase 5: Exemplar Ablation (~15 min)\n",
        "\n",
        "Tests the paper's claim: does the one-shot exemplar prompting trick help?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title **Phase 5: Exemplar Ablation Study**\n",
        "!python experiments/05_zero_shot_eval.py --experiment exemplar_ablation --max_samples 100 --quantize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title **View: Exemplar Ablation Results**\n",
        "import os, json\n",
        "\n",
        "path = \"results/tables/exemplar_ablation.json\"\n",
        "if os.path.exists(path):\n",
        "    with open(path) as f:\n",
        "        abl = json.load(f)\n",
        "    print(\"ONE-SHOT EXEMPLAR ABLATION\")\n",
        "    print(\"=\" * 55)\n",
        "    print(f\"{'Condition':<25} {'BLEU-1':>10} {'F1':>10}\")\n",
        "    print(\"-\" * 45)\n",
        "    for mode, m in abl.items():\n",
        "        print(f\"{mode:<25} {m['bleu_1']:>9.2f}% {m['f1']:>9.2f}%\")\n",
        "    diff = abl['with_exemplar']['bleu_1'] - abl['without_exemplar']['bleu_1']\n",
        "    ratio = abl['with_exemplar']['bleu_1'] / max(abl['without_exemplar']['bleu_1'], 0.001)\n",
        "    print(f\"\\nExemplar effect: {diff:+.2f}% BLEU-1 ({ratio:.1f}x improvement)\")\n",
        "    if diff > 0:\n",
        "        print(\"‚úÖ Confirms paper's finding: one-shot exemplars help across model scales\")\n",
        "    else:\n",
        "        print(\"Interesting: exemplar did not help for our model (still a valid finding)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Run Phase 5 first\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Final: Generate Comparison Report & Charts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title **Generate Final Comparison Table + Bar Charts**\n",
        "!python evaluation/compare_to_paper.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title **Display: Comparison Charts**\n",
        "import os, glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "charts = glob.glob(\"results/figures/*_comparison.png\")\n",
        "if charts:\n",
        "    for p in charts:\n",
        "        display(Image(filename=p, width=700))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No charts generated\")\n",
        "\n",
        "md_path = \"results/tables/full_comparison.md\"\n",
        "if os.path.exists(md_path):\n",
        "    print(\"\\n\" + open(md_path).read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Download Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title **Package & Download All Results**\n",
        "!tar -czf /content/reproduction_results.tar.gz results/\n",
        "from google.colab import files\n",
        "files.download('/content/reproduction_results.tar.gz')\n",
        "print(\"‚úÖ Results downloaded!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "| Aspect | Med-PaLM M | Our Reproduction |\n",
        "|--------|-----------|------------------|\n",
        "| Model | PaLM-E (562B) | BLIP-2 (~3B) |\n",
        "| Training | Full fine-tuning on TPU pods | LoRA on single T4 GPU |\n",
        "| Data | 1M+ samples across 14 tasks | ~3.5K VQA-RAD samples |\n",
        "| Compute | Weeks on TPU v4 | ~1 hour on free Colab |\n",
        "\n",
        "**Repository:** [github.com/Mrabbi3/biomed-multimodal-reproduction](https://github.com/Mrabbi3/biomed-multimodal-reproduction)"
      ]
    }
  ]
}